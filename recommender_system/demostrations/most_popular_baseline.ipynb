{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstration of a Simple Most Popular Baseline Recommender System\n",
    "Here the demonstration of the baseline model recommending the most popular news is presented.\n",
    "\n",
    "It simply recommends the most popular item in terms of the *click_count* per item.\n",
    "\n",
    "It also includes the evaluation of the recommender model using the metrics *Precision and Recall*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and setup\n",
    "import sys\n",
    "import os\n",
    "\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(parent_dir)\n",
    "from parquet_data_reader import ParquetDataReader\n",
    "from models.baseline.most_popular import MostPopularRecommender\n",
    "\n",
    "import polars as pl\n",
    "pl.Config.set_tbl_cols(-1)\n",
    "import numpy as np\n",
    "parquet_reader = ParquetDataReader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Extraction and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Split:\n",
      "Train shape: (98957, 4)\n",
      "Test shape: (42766, 4)\n",
      "\n",
      "Time-based Split:\n",
      "Train shape: (99207, 4)\n",
      "Test shape: (42516, 4)\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "from utils.baseline_processing import process_behavior_data, random_split, time_based_split\n",
    "\n",
    "train_behavior_df = parquet_reader.read_data(\"../../data/train/behaviors.parquet\")\n",
    "test_behaviours_df = parquet_reader.read_data('../../data/validation/behaviors.parquet')\n",
    "\n",
    "# Processes the data\n",
    "combined_df = process_behavior_data(train_behavior_df, test_behaviours_df)\n",
    "\n",
    "# ----- Method 1: Random Split -----\n",
    "train_random, test_random = random_split(combined_df, test_ratio=0.30)\n",
    "print(\"Random Split:\")\n",
    "print(\"Train shape:\", train_random.shape)\n",
    "print(\"Test shape:\", test_random.shape)\n",
    "\n",
    "# ----- Method 2: Time-based Split -----\n",
    "train_time, test_time = time_based_split(combined_df, test_ratio=0.30)\n",
    "print(\"\\nTime-based Split:\")\n",
    "print(\"Train shape:\", train_time.shape)\n",
    "print(\"Test shape:\", test_time.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Random Split of Train/Test for Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations for user 151570:\n",
      "shape: (5,)\n",
      "Series: 'article_id' [i32]\n",
      "[\n",
      "\t9773282\n",
      "\t9775562\n",
      "\t9776234\n",
      "\t9775776\n",
      "\t9786378\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Creates a recommender and fits it to the training data split using the random split method\n",
    "recommender = MostPopularRecommender(behaviors=train_random)\n",
    "recommender.fit()\n",
    "\n",
    "# Test user which is known to have interactions in the data\n",
    "user_id_test = 151570\n",
    "recommendations = recommender.recommend(user_id=user_id_test, n=5)\n",
    "\n",
    "print(f\"Recommendations for user {user_id_test}:\")\n",
    "print(recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Time-Based Split Train/Test Recommendations\n",
    "This methods splits the data into the oldest interactions *(test_ratio percent)*\n",
    "are used for testing, and the newest interactions are used for training. This happens after the total data (train and test) has been combined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations for user 151570:\n",
      "shape: (5,)\n",
      "Series: 'article_id' [i32]\n",
      "[\n",
      "\t9776234\n",
      "\t9787465\n",
      "\t9785668\n",
      "\t9780195\n",
      "\t9786378\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Creates a recommender and fits it to the training data split using the time split method\n",
    "recommender2 = MostPopularRecommender(behaviors=train_time)\n",
    "recommender2.fit()\n",
    "\n",
    "recommendations2 = recommender2.recommend(user_id=user_id_test, n=5)\n",
    "\n",
    "print(f\"Recommendations for user {user_id_test}:\")\n",
    "print(recommendations2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison: Evaluation of the Most Popular (Baseline) Recommender\n",
    "Comparing the two different data-splits for this most popular recommender using the metrics *Precision and Recall*.\n",
    "*FPR* is also printed for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation metrics (precision and recall at k):\n",
      "{'precision@k': np.float64(0.012832102304045861), 'recall@k': np.float64(0.019111413300729043), 'fpr@k': np.float64(0.0021611723302661363)}\n",
      "\n",
      "Evaluation metrics (precision and recall at k):\n",
      "{'precision@k': np.float64(0.0), 'recall@k': np.float64(0.0), 'fpr@k': np.float64(0.004364917949446781)}\n"
     ]
    }
   ],
   "source": [
    "from utils.evaluation import perform_model_evaluation\n",
    "\n",
    "# Evaluates the recommender using the same data as test data\n",
    "metrics = perform_model_evaluation(recommender, test_data=test_random, k=5)\n",
    "print(\"\\nEvaluation metrics (precision and recall at k):\")\n",
    "print(metrics)\n",
    "\n",
    "\n",
    "# Evaluates the recommender using the same data as test data\n",
    "metrics2 = perform_model_evaluation(recommender2, test_data=test_time, k=5)\n",
    "print(\"\\nEvaluation metrics (precision and recall at k):\")\n",
    "print(metrics2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.evaluation import append_model_metrics\n",
    "append_model_metrics(metrics, \"most_pop_baseline_random_split\")\n",
    "append_model_metrics(metrics2, \"most_pop_baseline_time_split\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Diversity Evaluation\n",
    "\n",
    "Calculates the aggrigate diversity of the recommender model recommendations, and appends the result to the `/evaluation_summary/model_overview_diversity.csv`-file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diversity Random Split\n",
      "0.0017193947730398899\n",
      "Diversity Time Split\n",
      "0.0017193947730398899\n"
     ]
    }
   ],
   "source": [
    "from utils.evaluation import aggregate_diversity\n",
    "from utils.evaluation import append_aggregate_diversity\n",
    "\n",
    "# For the random split model\n",
    "diversity = aggregate_diversity(recommender, combined_df, user_sample=1000)\n",
    "\n",
    "print(\"Diversity Random Split\")\n",
    "print(diversity)\n",
    "\n",
    "append_aggregate_diversity(diversity, \"most_popular_random\")\n",
    "\n",
    "diversity2 = aggregate_diversity(recommender2, combined_df, user_sample=1000)\n",
    "\n",
    "# For the time split rec model\n",
    "print(\"Diversity Time Split\")\n",
    "print(diversity2)\n",
    "\n",
    "append_aggregate_diversity(diversity2, \"most_popular_time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carbon Footprint\n",
    "This section creates an emissions.csv file in the \"output\"-folder\n",
    "It utilizes the code_carbon (`codecarbon EmissionsTracker`) to record the carbon footprint of the `fit` and the `recommend` methods of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 15:50:11] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 15:50:11] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 15:50:11] No CPU tracking mode found. Falling back on CPU constant mode. \n",
      " Windows OS detected: Please install Intel Power Gadget to measure CPU\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Carbon footprint of the recommender:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 15:50:13] We saw that you have a 13th Gen Intel(R) Core(TM) i7-13700H but we don't know it. Please contact us.\n",
      "[codecarbon INFO @ 15:50:13] CPU Model on constant consumption mode: 13th Gen Intel(R) Core(TM) i7-13700H\n",
      "[codecarbon INFO @ 15:50:13] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 15:50:13] No GPU found.\n",
      "[codecarbon INFO @ 15:50:13] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 15:50:13]   Platform system: Windows-10-10.0.26100-SP0\n",
      "[codecarbon INFO @ 15:50:13]   Python version: 3.11.9\n",
      "[codecarbon INFO @ 15:50:13]   CodeCarbon version: 2.8.3\n",
      "[codecarbon INFO @ 15:50:13]   Available RAM : 15.731 GB\n",
      "[codecarbon INFO @ 15:50:13]   CPU count: 20\n",
      "[codecarbon INFO @ 15:50:13]   CPU model: 13th Gen Intel(R) Core(TM) i7-13700H\n",
      "[codecarbon INFO @ 15:50:13]   GPU count: None\n",
      "[codecarbon INFO @ 15:50:13]   GPU model: None\n",
      "[codecarbon INFO @ 15:50:16] Saving emissions data to file c:\\Users\\magnu\\NewDesk\\An.sys\\TDT4215\\recommender_system\\demostrations\\output\\most_popular_fit_emission.csv\n",
      "[codecarbon INFO @ 15:50:16] Energy consumed for RAM : 0.000000 kWh. RAM Power : 5.899243354797363 W\n",
      "[codecarbon INFO @ 15:50:16] Energy consumed for all CPUs : 0.000000 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 15:50:16] 0.000000 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 15:50:16] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 15:50:16] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 15:50:16] No CPU tracking mode found. Falling back on CPU constant mode. \n",
      " Windows OS detected: Please install Intel Power Gadget to measure CPU\n",
      "\n",
      "[codecarbon WARNING @ 15:50:18] We saw that you have a 13th Gen Intel(R) Core(TM) i7-13700H but we don't know it. Please contact us.\n",
      "[codecarbon INFO @ 15:50:18] CPU Model on constant consumption mode: 13th Gen Intel(R) Core(TM) i7-13700H\n",
      "[codecarbon INFO @ 15:50:18] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 15:50:18] No GPU found.\n",
      "[codecarbon INFO @ 15:50:18] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 15:50:18]   Platform system: Windows-10-10.0.26100-SP0\n",
      "[codecarbon INFO @ 15:50:18]   Python version: 3.11.9\n",
      "[codecarbon INFO @ 15:50:18]   CodeCarbon version: 2.8.3\n",
      "[codecarbon INFO @ 15:50:18]   Available RAM : 15.731 GB\n",
      "[codecarbon INFO @ 15:50:18]   CPU count: 20\n",
      "[codecarbon INFO @ 15:50:18]   CPU model: 13th Gen Intel(R) Core(TM) i7-13700H\n",
      "[codecarbon INFO @ 15:50:18]   GPU count: None\n",
      "[codecarbon INFO @ 15:50:18]   GPU model: None\n",
      "[codecarbon INFO @ 15:50:21] Saving emissions data to file c:\\Users\\magnu\\NewDesk\\An.sys\\TDT4215\\recommender_system\\demostrations\\output\\most_popular_recommend_emission.csv\n",
      "[codecarbon INFO @ 15:50:21] Energy consumed for RAM : 0.000000 kWh. RAM Power : 5.899243354797363 W\n",
      "[codecarbon INFO @ 15:50:21] Energy consumed for all CPUs : 0.000000 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 15:50:21] 0.000000 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fit': (None, 6.2988015538794525e-09),\n",
       " 'recommend': (shape: (5,)\n",
       "  Series: 'article_id' [i32]\n",
       "  [\n",
       "  \t9773282\n",
       "  \t9775562\n",
       "  \t9776234\n",
       "  \t9775776\n",
       "  \t9786378\n",
       "  ],\n",
       "  1.2178898341192162e-08)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.evaluation import record_carbon_footprint, track_model_energy\n",
    "\n",
    "# Records the carbon footprint of the recommender\n",
    "#carbon_footprint = record_carbon_footprint(recommender.recommend, user_id=user_id_test, n=5)\n",
    "\n",
    "print(\"\\nCarbon footprint of the recommender:\")\n",
    "footprint = track_model_energy(recommender, \"most_popular\", user_id=user_id_test, n=5)\n",
    "footprint"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
